{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 探索数据\n",
    "\n",
    "\n",
    "数据集使用 kaggle 数据集，解压后存放在images目录下\n",
    "- 训练数据集路径：\n",
    "images/train\n",
    "- 测试数据集路径：\n",
    "images/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 整理数据集\n",
    " \n",
    "这里由于 trian 文件夹下的猫和狗的数据是混在一起的，需要将猫和狗的图片分别存储。\n",
    "\n",
    "图片的命名规则：类别.编码.jpg\n",
    "\n",
    "**注意：这里的处理只需要运行一次即可**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "# sys.path.append(\"/home/ubuntu/anaconda3/envs/dog-project/lib/python3.6/site-packages\")\n",
    "# sys.path.append(\"/home/ubuntu/anaconda3/pkgs/tensorflow-gpu-base-1.3.0-py36cuda8.0cudnn6.0_0/lib/python3.6/site-packages\")\n",
    "# sys.path.append(\"/home/ubuntu/anaconda3/pkgs/tensorflow-gpu-base-1.3.0-py36cuda8.0cudnn6.0_0/lib/python3.6/site-packages/tensorflow/contrib/keras/api\")\n",
    "# sys.path.append(\"/home/ubuntu/anaconda3/pkgs/tensorflow-gpu-base-1.3.0-py36cuda8.0cudnn6.0_0/lib/python3.6/site-packages\")\n",
    "# print(sys.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy.stats import mode as spmode\n",
    "import platform\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers import GlobalAveragePooling2D,Dense\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "\n",
    "print(platform.system())\n",
    "\n",
    "notmatch_path='images/notmatchs'\n",
    "notmatch_txt = 'notmatchs.txt'\n",
    "all_path = 'images/all'\n",
    "train_path = 'images/all/trainnew'\n",
    "dog_path = 'images/all/trainnew/dogs'\n",
    "\n",
    "\n",
    "testnew_path = 'images/all/testnew'\n",
    "test_path = 'images/all/testnew/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载特征向量\n",
    "# !wget -P saved_models/  https://github.com/ypwhs/dogs_vs_cats/releases/download/gap/gap_InceptionV3.h5\n",
    "# !wget -P saved_models/  https://github.com/ypwhs/dogs_vs_cats/releases/download/gap/gap_ResNet50.h5\n",
    "# !wget -P saved_models/  https://github.com/ypwhs/dogs_vs_cats/releases/download/gap/gap_Xception.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows分隔符\n",
    "splitflag = \"\\\\\"\n",
    "if (platform.system() == 'Linux'):\n",
    "    splitflag = \"/\"\n",
    "\n",
    "#处理的样本量\n",
    "train_limitcount = 12500\n",
    "test_limitcount = 12500 #不限制个数全部使用\n",
    "\n",
    "def create_dir(path):\n",
    "    '''\n",
    "    创建文件夹\n",
    "    '''\n",
    "    if (os.path.exists(path) == False):\n",
    "        os.mkdir (path)\n",
    "    else:\n",
    "        print('文件夹已经存在：%s' % path)\n",
    "        shutil.rmtree(path)\n",
    "        os.mkdir (path)\n",
    "\n",
    "# 限制训练文件数量,0不限制\n",
    "def copy_image_bytype(srcpath, destpath, limitcount = 0):\n",
    "    '''\n",
    "    将猫狗文件分别拷贝到不同文件夹下\n",
    "    '''\n",
    "    \n",
    "    filenames = [item.split(splitflag)[-1] for item in sorted(srcpath)]\n",
    "    \n",
    "    count = 0\n",
    "    for src,name in zip(srcpath,filenames):\n",
    "        dst = destpath + \"/\" + name\n",
    "        if (os.path.exists(dst) == False):\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "        count += 1\n",
    "        \n",
    "        # 跳出循环\n",
    "        if (limitcount == count):\n",
    "            break\n",
    "            \n",
    "    return count\n",
    "\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹已经存在：images/notmatchs\n",
      "文件夹已经存在：images/all\n"
     ]
    }
   ],
   "source": [
    "# 创建文件夹\n",
    "create_dir(notmatch_path)\n",
    "create_dir(all_path)\n",
    "\n",
    "create_dir(train_path)\n",
    "create_dir(dog_path)\n",
    "\n",
    "\n",
    "create_dir(testnew_path)\n",
    "create_dir(test_path)\n",
    "\n",
    "\n",
    "cat_path = 'images/all/trainnew/cats'\n",
    "create_dir(cat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 total dog categories.\n"
     ]
    }
   ],
   "source": [
    "# 处理狗\n",
    "dogs_all = glob(\"images/train/dog.*\")\n",
    "\n",
    "# 整理狗数据\n",
    "count = copy_image_bytype(dogs_all, dog_path, train_limitcount)\n",
    "\n",
    "print('There are %d total dog categories.' % count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 total cat categories.\n"
     ]
    }
   ],
   "source": [
    "# 处理猫\n",
    "cats_all = glob(\"images/train/cat.*\")\n",
    "\n",
    "# 整理猫数据\n",
    "count = copy_image_bytype(cats_all, cat_path, train_limitcount)\n",
    "\n",
    "print('There are %d total cat categories.' % count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 total test categories.\n"
     ]
    }
   ],
   "source": [
    "# 处理测试数据\n",
    "test_all = glob(\"images/test/*\")\n",
    "\n",
    "# 整理猫数据\n",
    "count = copy_image_bytype(test_all, test_path, test_limitcount)\n",
    "\n",
    "print('There are %d total test categories.' % count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "#当前使用的样例\n",
    "dogs_list = glob(dog_path+'/dog.*')\n",
    "print(len(dogs_list))\n",
    "\n",
    "test_list = glob(test_path+'/*')\n",
    "print(len(test_list))\n",
    "\n",
    "cats_list = glob(cat_path+'/cat.*')\n",
    "print(len(cats_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5b634b2a9e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filesizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m     \u001b[0m获取文件大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_filesizes(paths):\n",
    "    '''\n",
    "    获取文件大小\n",
    "    '''\n",
    "    filesizes = []\n",
    "    for path in paths:\n",
    "        size = os.path.getsize(path)\n",
    "        filesizes.append(size)\n",
    "    return filesizes\n",
    "\n",
    "def get_filepixs(paths):\n",
    "    '''\n",
    "    获取像素：长和宽\n",
    "    '''\n",
    "    filepixs = []\n",
    "    xpixs = []\n",
    "    ypixs = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path)\n",
    "        pix = img.size #图片的长和宽\n",
    "        xpixs.append(pix[0]) #图片的宽\n",
    "        ypixs.append(pix[1]) #图片的长\n",
    "        if (pix[0] > 1000):\n",
    "            print(path)\n",
    "    filepixs.append(xpixs)\n",
    "    filepixs.append(ypixs)    \n",
    "    \n",
    "    return filepixs\n",
    "    \n",
    "def draw_hist(mylist, title, xlabel, ylabel, xmin, xmax, ymin, ymax):\n",
    "    '''\n",
    "    绘制直方图，参数依次为list,抬头,X轴标签,Y轴标签,XY轴的范围\n",
    "    '''\n",
    "    plt.hist(mylist, 100)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_scatter(x1, y1, title, label):\n",
    "    '''\n",
    "    绘制像素的散点图\n",
    "    '''\n",
    "    plt.scatter(x1, y1, marker = 'x',color = 'red', s = 40 ,label = label)\n",
    "    plt.xlabel('wide')\n",
    "    plt.ylabel('length')\n",
    "    plt.title(title)\n",
    "    \n",
    "#     plt.xlim(-1.5, 1.5)\n",
    "#     plt.xticks(())  # ignore xticks\n",
    "#     plt.ylim(-1.5, 1.5)\n",
    "#     plt.yticks(())  # ignore yticks\n",
    "\n",
    "    plt.show()   \n",
    "\n",
    "    \n",
    "def show_filesize(paths):\n",
    "    '''\n",
    "    展示文件大小分布\n",
    "    '''\n",
    "    filesizes = get_filesizes(paths)\n",
    "\n",
    "    # train 中文件的的最小值、最大值、中位数、众数\n",
    "    train_min = np.min(filesizes)\n",
    "    print('train 中文件的的最小值:', train_min)\n",
    "    train_max = np.max(filesizes)\n",
    "    print('train 中文件的的最大值:', train_max)\n",
    "    train_median = np.median(filesizes)\n",
    "    print('train 中文件的的中位数:', train_median)\n",
    "    train_mode = spmode(filesizes)\n",
    "    print('train 中文件的的众数:', train_mode)\n",
    "        \n",
    "    title = 'file size distribute'\n",
    "    xlabel = 'file size'\n",
    "    ylabel = 'file count'\n",
    "    xmin = np.min(filesizes)\n",
    "    xmax = np.max(filesizes)\n",
    "    ymin = 0\n",
    "    ymax = 1200\n",
    "    draw_hist(filesizes, title, xlabel, ylabel, xmin, xmax, ymin, ymax)\n",
    "\n",
    "# 训练数据集文件大小的分布\n",
    "# show_filesize(dogs_list)        \n",
    "# 测试数据集文件大小的分布\n",
    "# show_filesize(cats_list)          \n",
    "    \n",
    "def show_filepix(paths):\n",
    "    '''\n",
    "    显示像素分布\n",
    "    '''\n",
    "    filepixs = get_filepixs(paths)\n",
    "    title = 'file pix distribute'\n",
    "    draw_scatter(filepixs[0], filepixs[1], title, 'dog')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22df80205b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 狗的数据集图片大小分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_filepix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdogs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b1e1987926ab>\u001b[0m in \u001b[0;36mshow_filepix\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0m显示像素分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     '''\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mfilepixs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file pix distribute'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mdraw_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b1e1987926ab>\u001b[0m in \u001b[0;36mget_filepixs\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mypixs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;31m#图片的长和宽\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mxpixs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#图片的宽\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# 狗的数据集图片大小分布\n",
    "\n",
    "show_filepix(dogs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 猫的数据集图片大小分布\n",
    "show_filepix(cats_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 异常数据清理\n",
    "\n",
    "异常数据有:\n",
    "\n",
    "标签与图片信息不相符的，例如：虽然标识了是狗的图片，但是图片中没有狗。\n",
    "\n",
    "照片的背景过于复杂，识别起来很困难，例如：虽然有狗但是有很多干扰元素。\n",
    "\n",
    "这里选择4种算法:ResNet50, VGG19, Xception, InceptionV3 获取的并集进行数据清理\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "#可能概率前几个类型\n",
    "topparma = 50\n",
    "# 筛选的数量\n",
    "doglimit = train_limitcount\n",
    "catlimit = train_limitcount\n",
    "\n",
    "\n",
    "# 对 ImageNet 分类进行分析\n",
    "# 狗分类：118种，猫分类：7种\n",
    "\n",
    "ImageNetdogs = [\n",
    " 'n02085620','n02085782','n02085936','n02086079'\n",
    ",'n02086240','n02086646','n02086910','n02087046'\n",
    ",'n02087394','n02088094','n02088238','n02088364'\n",
    ",'n02088466','n02088632','n02089078','n02089867'\n",
    ",'n02089973','n02090379','n02090622','n02090721'\n",
    ",'n02091032','n02091134','n02091244','n02091467'\n",
    ",'n02091635','n02091831','n02092002','n02092339'\n",
    ",'n02093256','n02093428','n02093647','n02093754'\n",
    ",'n02093859','n02093991','n02094114','n02094258'\n",
    ",'n02094433','n02095314','n02095570','n02095889'\n",
    ",'n02096051','n02096177','n02096294','n02096437'\n",
    ",'n02096585','n02097047','n02097130','n02097209'\n",
    ",'n02097298','n02097474','n02097658','n02098105'\n",
    ",'n02098286','n02098413','n02099267','n02099429'\n",
    ",'n02099601','n02099712','n02099849','n02100236'\n",
    ",'n02100583','n02100735','n02100877','n02101006'\n",
    ",'n02101388','n02101556','n02102040','n02102177'\n",
    ",'n02102318','n02102480','n02102973','n02104029'\n",
    ",'n02104365','n02105056','n02105162','n02105251'\n",
    ",'n02105412','n02105505','n02105641','n02105855'\n",
    ",'n02106030','n02106166','n02106382','n02106550'\n",
    ",'n02106662','n02107142','n02107312','n02107574'\n",
    ",'n02107683','n02107908','n02108000','n02108089'\n",
    ",'n02108422','n02108551','n02108915','n02109047'\n",
    ",'n02109525','n02109961','n02110063','n02110185'\n",
    ",'n02110341','n02110627','n02110806','n02110958'\n",
    ",'n02111129','n02111277','n02111500','n02111889'\n",
    ",'n02112018','n02112137','n02112350','n02112706'\n",
    ",'n02113023','n02113186','n02113624','n02113712'\n",
    ",'n02113799','n02113978']\n",
    "\n",
    "ImageNetcats=[\n",
    "'n02123045','n02123159','n02123394','n02123597'\n",
    ",'n02124075','n02125311','n02127052']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_animalarray(model, top, animalarray,img_paths, xsize, ysize, preprocess_input, decode_predictions):\n",
    "    '''\n",
    "    清理数据集中不是对应标签的文件\n",
    "    model:模型\n",
    "    top:可能概率前几个类型\n",
    "    animalarray: ImageNet分类集\n",
    "    img_paths:数据集路径\n",
    "    x:图片宽\n",
    "    y:图片高\n",
    "    '''\n",
    "    animals = []\n",
    "    for img_path in img_paths:\n",
    "        img = image.load_img(img_path, target_size=(xsize, ysize))\n",
    "        imgarray = image.img_to_array(img)\n",
    "        imgarray = np.expand_dims(imgarray, axis=0)\n",
    "        imgarray = preprocess_input(imgarray)\n",
    "\n",
    "        preds = model.predict(imgarray)\n",
    "        predarray = decode_predictions(preds, top=top)\n",
    "#         sorted(tmparray, key=lambda tmp: tmp[2], reverse=True) \n",
    "\n",
    "        # 判断是否有指定类型存在\n",
    "        existflag = False\n",
    "        for n in range(top):\n",
    "            animal = predarray[0][n][0]\n",
    "            if animal in animalarray:\n",
    "                existflag = True\n",
    "                break\n",
    "        if existflag == False:\n",
    "            animals.append(img_path)\n",
    "            \n",
    "    return animals\n",
    "\n",
    "# [[('n02092002', 'Scottish_deerhound', 0.21568948),('n02097130', 'giant_schnauzer', 0.074100845)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用ResNet50网络进行ImageNet分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "ResNet50_model = ResNet50(weights='imagenet')\n",
    "\n",
    "print(doglimit)\n",
    "print(catlimit)\n",
    "\n",
    "# # 不是狗的\n",
    "# ResNet50_notdogs = get_not_animalarray(ResNet50_model, topparma, ImageNetdogs, dogs_list[0:doglimit], 224, 224, preprocess_input, decode_predictions)\n",
    "# print(ResNet50_notdogs)\n",
    "\n",
    "# # 不是猫的\n",
    "# ResNet50_notcats = get_not_animalarray(ResNet50_model, topparma, ImageNetcats, cats_list[0:catlimit], 224, 224, preprocess_input, decode_predictions)\n",
    "# print(ResNet50_notdogs)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用VGG19网络进行ImageNet分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input,decode_predictions\n",
    "\n",
    "VGG19_model = VGG19(weights='imagenet')\n",
    "\n",
    "# # 不是狗的\n",
    "# VGG19_notdogs = get_not_animalarray(VGG19_model, topparma, ImageNetdogs, dogs_list[0:doglimit], 224, 224, preprocess_input, decode_predictions)\n",
    "# print(VGG19_notdogs)\n",
    "\n",
    "# # 不是猫的\n",
    "# VGG19_notcats = get_not_animalarray(VGG19_model, topparma, ImageNetcats, cats_list[0:catlimit], 224, 224, preprocess_input, decode_predictions)\n",
    "# print(VGG19_notcats)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用Xception网络进行ImageNet分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input,decode_predictions\n",
    "\n",
    "Xception_model = Xception(weights='imagenet')\n",
    "\n",
    "# # 不是狗的\n",
    "# Xception_notdogs = get_not_animalarray(Xception_model, topparma, ImageNetdogs, dogs_list[0:doglimit], 299, 299, preprocess_input, decode_predictions)\n",
    "# print(Xception_notdogs)\n",
    "\n",
    "# # 不是猫的\n",
    "# Xception_notcats = get_not_animalarray(Xception_model, topparma, ImageNetcats, cats_list[0:catlimit], 299, 299, preprocess_input, decode_predictions)\n",
    "# print(Xception_notcats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用InceptionV3网络进行ImageNet分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input,decode_predictions\n",
    "\n",
    "InceptionV3_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# # 不是狗的\n",
    "# InceptionV3_notdogs = get_not_animalarray(InceptionV3_model, topparma, ImageNetdogs, dogs_list[0:doglimit], 299, 299, preprocess_input, decode_predictions)\n",
    "# print(InceptionV3_notdogs)\n",
    "\n",
    "# # 不是猫的\n",
    "# InceptionV3_notcats = get_not_animalarray(InceptionV3_model, topparma, ImageNetcats, cats_list[0:catlimit], 299, 299, preprocess_input, decode_predictions)\n",
    "# print(InceptionV3_notcats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并结果集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 合并所有的结果，取并集\n",
    "# notdogs = ResNet50_notdogs + VGG19_notdogs + Xception_notdogs + InceptionV3_notdogs\n",
    "# notdogs = list(set(notdogs))\n",
    "# print(notdogs)\n",
    "\n",
    "# notcats = ResNet50_notcats + VGG19_notcats + Xception_notcats + InceptionV3_notdogs\n",
    "# notcats = list(set(notcats))\n",
    "# print(notcats)\n",
    "\n",
    "\n",
    "# notmatchs = notdogs + notcats\n",
    "# print(len(notmatchs))\n",
    "# # notmatchs = notdogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存异常数据列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存异常数据列表\n",
    "# f = open('notmatchs.txt','w')\n",
    "# f.write(','.join(notmatchs))\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 剔除异常数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "def show_multiple_image(images):\n",
    "    '''\n",
    "    展示多组图片\n",
    "    '''\n",
    "    count = 1\n",
    "    row = len(images) / 4\n",
    "    col = 4\n",
    "    if (row < 1):\n",
    "        row = 1\n",
    "    for image in images:\n",
    "        plt.figure(num='astronaut', figsize =(12,15))  #创建一个名为astronaut的窗口,并设置大小 \n",
    "        img = plt.imread(image)  \n",
    "        if (count == 1):\n",
    "            plt.subplot(row,col,count)     #将窗口分为两行两列四个子图，则可显示四幅图片\n",
    "            plt.title(image.split(splitflag)[-1])   #第一幅图片标题l\n",
    "            plt.imshow(img)      #绘制第一幅图片\n",
    "        else:\n",
    "            plt.subplot(row,col,count)     #第三个子图\n",
    "            plt.title(image.split(splitflag)[-1])   #第一幅图片标题l\n",
    "            plt.imshow(img)      #绘制第一幅图片\n",
    "\n",
    "        if (count >= 8):\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    plt.show()   #显示窗口\n",
    "\n",
    "\n",
    "# test1=['images/all/testnew/test\\\\1.jpg', 'images/all/testnew/test\\\\10.jpg', 'images/all/testnew/test\\\\100.jpg', 'images/all/testnew/test\\\\1000.jpg']    \n",
    "# test2=['images/all/testnew/test\\\\1.jpg', 'images/all/testnew/test\\\\10.jpg', 'images/all/testnew/test\\\\100.jpg', 'images/all/testnew/test\\\\1000.jpg']    \n",
    "# test3=['images/all/testnew/test\\\\1.jpg', 'images/all/testnew/test\\\\10.jpg', 'images/all/testnew/test\\\\100.jpg', 'images/all/testnew/test\\\\1000.jpg']    \n",
    "# test4=['images/all/testnew/test\\\\101.jpg', 'images/all/testnew/test\\\\102.jpg', 'images/all/testnew/test\\\\100.jpg', 'images/all/testnew/test\\\\1000.jpg']    \n",
    "# test5=[]\n",
    "# test = test1+ test2 + test3 + test4\n",
    "# print(len(notmatchs))\n",
    "# show_multiple_image(notmatchs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_as_str(file_path):\n",
    "    # 判断路径文件存在\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise TypeError(file_path + \" does not exist\")\n",
    "\n",
    "    all_the_text = open(file_path).read()\n",
    "    # print type(all_the_text)\n",
    "    return all_the_text\n",
    "\n",
    "\n",
    "def file_move(srcpath, despath):\n",
    "    filenames = [item.split(splitflag)[-1] for item in sorted(srcpath)]\n",
    "    count = len(filenames)\n",
    "        \n",
    "    for src,name in zip(srcpath,filenames):\n",
    "        dst = despath + \"/\" + name\n",
    "        if (os.path.exists(src) == True):\n",
    "            shutil.move(src, dst)\n",
    "            \n",
    "    return count\n",
    "        \n",
    "notmatch_str = read_file_as_str(notmatch_txt)\n",
    "notmatchs = []\n",
    "notmatchs = notmatch_str.strip(',').split(',')\n",
    "# print(notmatch_txt)\n",
    "# print(notmatchs[0])\n",
    "# print(notmatch_path)  \n",
    "    \n",
    "\n",
    "# 剔除异常数据\n",
    "file_move(notmatchs, notmatch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers import GlobalAveragePooling2D,Dense\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 train 测试数据集\n",
    "train_data = load_files(train_path)\n",
    "\n",
    "# 加载 test 测试数据集\n",
    "test_data = load_files(testnew_path)    \n",
    "\n",
    "trainall_file = np.array(train_data['filenames'])\n",
    "trainall_targets = np_utils.to_categorical(np.array(train_data['target']), 2)\n",
    "trainall_targets = np.array(trainall_targets)[:,1]\n",
    "# print(type(train_data))\n",
    "print(trainall_file)\n",
    "print(trainall_targets)\n",
    "\n",
    "# 展示训练数据集\n",
    "print('训练集数据总数：%d' % len(trainall_file))\n",
    "\n",
    "\n",
    "# 将原来的训练数据分割成训练数据集和验证数据集，比例为20%\n",
    "train_files, valid_files, mytrain_targets, myvalid_targets = train_test_split(trainall_file, trainall_targets, test_size=0.2)\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(valid_files))\n",
    "print(mytrain_targets)\n",
    "print(myvalid_targets)\n",
    "\n",
    "\n",
    "test_files = np.array(test_data['filenames'])\n",
    "test_targets = np_utils.to_categorical(np.array(test_data['target']), 1)\n",
    "\n",
    "# 展示测试数据集\n",
    "print('测试集数据总数：%d' % len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "\n",
    "# sys.modules['Image'] = Image \n",
    "# from PIL import Image\n",
    "# print(Image.__file__)\n",
    "# import Image\n",
    "# print(Image.__file__)\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# def path_to_tensor(img_path):\n",
    "# #     print(img_path)\n",
    "#     # 用PIL加载RGB图像为PIL.Image.Image类型\n",
    "#     img = image_utils.load_img(img_path,target_size=(299,299))\n",
    "    \n",
    "#     #将PIL.Image.Image类型转化为格式为(299,299,3)的3维张量\n",
    "#     x = image_utils.img_to_array(img)\n",
    "    \n",
    "#     #将3维张量转化为格式为(1, 299, 299, 3) 的4维张量并返回\n",
    "#     return np.expand_dims(x, axis=0)\n",
    "\n",
    "# def paths_to_tensor(img_paths):\n",
    "#     list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "#     return np.vstack(list_of_tensors)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    gen = ImageDataGenerator()\n",
    "    generator = gen.flow_from_directory(img_paths, (299,299), shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    return generator\n",
    "\n",
    "# img_path='images/all/train/trainnew/cats/cat.4424.jpg'\n",
    "# paths_to_tensor(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "# img_path='images/all/trainnew/cats/cat.1584.jpg'\n",
    "# img = load_img(img_path,target_size=(299,299))\n",
    "# # img = Image.open(img_path)\n",
    "# #将PIL.Image.Image类型转化为格式为(299,299,3)的3维张量\n",
    "# x = img_to_array(img)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 训练集预处理\n",
    "# mytrain_tensors = paths_to_tensor(train_files).astype('float32')/127.5 - 1\n",
    "# print(mytrain_tensors.shape)\n",
    "\n",
    "mytrain_tensors = paths_to_tensor(train_path)\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 验证集预处理\n",
    "# myvalid_tensors = paths_to_tensor(valid_files).astype('float32')/127.5 - 1\n",
    "\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 测试集预处理\n",
    "# mytest_tensors = paths_to_tensor(test_files).astype('float32')/127.5 - 1\n",
    "mytest_tensors = paths_to_tensor(testnew_path)\n",
    "\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 导出特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出特征向量\n",
    "def write_gap(MODEL, image_size, lambda_func=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(train_path, image_size, shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(testnew_path, image_size, shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "\n",
    "    nb_sample = len(train_generator.filenames)\n",
    "    print('nb_sample:',nb_sample)\n",
    "    train = model.predict_generator(train_generator, len(train_generator.filenames), verbose=1)\n",
    "    nb_sample = len(test_generator.filenames)\n",
    "    test = model.predict_generator(test_generator, len(test_generator.filenames), verbose=1)\n",
    "    with h5py.File(\"gap_%s.h5\"%MODEL.func_name) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "\n",
    "write_gap(ResNet50, (224, 224))\n",
    "write_gap(InceptionV3, (299, 299), inception_v3.preprocess_input)\n",
    "write_gap(Xception, (299, 299), xception.preprocess_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 构建模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 训练模型\n",
    "\n",
    "训练 InceptionV3 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入特征向量\n",
    "train_tensors = []\n",
    "test_tensors = []\n",
    "\n",
    "for filename in [\"saved_models/gap_ResNet50.h5\", \"saved_models/gap_Xception.h5\", \"saved_models/gap_InceptionV3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        train_tensors.append(np.array(h['train']))\n",
    "        test_tensors.append(np.array(h['test']))\n",
    "        train_targets = np.array(h['label'])\n",
    "\n",
    "train_tensors = np.concatenate(train_tensors, axis=1)\n",
    "test_tensors = np.concatenate(test_tensors, axis=1)\n",
    "train_tensors, train_targets = shuffle(train_tensors, train_targets)\n",
    "\n",
    "# print(train_tensors)\n",
    "print(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "patience=10\n",
    "\n",
    "# logloss 趋势图\n",
    "def show_logloss(history_callback):\n",
    "    plt.plot(history_callback.history['loss'])\n",
    "    plt.plot(history_callback.history['val_loss'])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\",\"test\"],loc=\"upper left\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 准确率趋势图\n",
    "def show_acc(history_callback):\n",
    "    plt.plot(history_callback.history['acc'])\n",
    "    plt.plot(history_callback.history['val_acc'])\n",
    "    plt.title(\"model acc\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\",\"test\"],loc=\"upper left\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 1.构建不带分类器的预训练模型\n",
    "base_model = InceptionV3(weights='imagenet',include_top=False) \n",
    "\n",
    "# 2.添加全局平均池化层\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# 3.全连接层，可选，如果精度够用则可以不加\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# 4. 添加一个分类器，使用 1 个神经元，sigmoid激活函数\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# 5. 构建我们需要训练的完整模型\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# 6.首先只训练顶部的几层（随机初始化的层），锁住所有 InceptionV3d 卷积层\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 7.编译模型（一定要在锁层以后操作）\n",
    "# model.compile(optimizer='adadelta', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('compile ok')\n",
    "\n",
    "\n",
    "# 8. 在新的数据集上训练几代\n",
    "# early stoppping \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# monitor: 需要监视的变量\n",
    "# patience:在发现变量没有变化后的多少个epoch停止\n",
    "# verbose:信息展示模式\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "callback_lists = [tensorboard, early_stopping]\n",
    "\n",
    "history_callback_1 = model.fit(mytrain_tensors, mytrain_targets, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=0.2, verbose=1, callbacks=callback_lists)\n",
    "\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(train_tensors.shape[1:])\n",
    "x = Dropout(0.5)(input_tensor)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=input_tensor, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('compile ok')\n",
    "\n",
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 8. 在新的数据集上训练几代\n",
    "# early stoppping \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# monitor: 需要监视的变量\n",
    "# patience:在发现变量没有变化后的多少个epoch停止\n",
    "# verbose:信息展示模式\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "callback_lists = [tensorboard, early_stopping]\n",
    "\n",
    "history_callback_1 = model.fit(train_tensors, train_targets, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=0.2, verbose=1, callbacks=callback_lists)\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示logloss趋势\n",
    "show_logloss(history_callback_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.现在顶层应该训练好了，开始微调 InceptionV3的卷积层。\n",
    "#锁住底下的几层，然后训练其余的顶层。查看每一层的名字和层号，看看应该锁多少层\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name,layer.trainable)\n",
    "    \n",
    "# conv2d_841 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.我们选择训练最上面的两个 Inception block, 锁住前面249层，然后放开之后的层\n",
    "# for layer in model.layers[:249]:\n",
    "#     print('before:',layer.name,layer.trainable)\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[249:]:\n",
    "#     print('after:',layer.name,layer.trainable)\n",
    "#     layer.trainable = True    \n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = True\n",
    "    print('before:',layer.name,layer.trainable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# 11.重新编译模型，使上面的修改生效，设置一个很低的学习率:lr=0.001，使用SGD来微调\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0., decay=0., nesterov=False), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('compile ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.strftime('start time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (starttime)\n",
    "\n",
    "# 12.继续训练模型，训练最后两个 Inception block 和两个全连接层\n",
    "Inceptionfile_hdf5 ='saved_models/weights.best.Inception.hdf5'\n",
    "\n",
    "# 模型保存\n",
    "checkpointer = ModelCheckpoint(filepath=Inceptionfile_hdf5, verbose=1, save_best_only=True)\n",
    "\n",
    "# 可视化\n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "\n",
    "# 自动停止训练\n",
    "# monitor: 需要监视的变量\n",
    "# patience:在发现变量没有变化后的多少个epoch停止\n",
    "# verbose:信息展示模式\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "\n",
    "\n",
    "callback_lists = [tensorboard, checkpointer, early_stopping]\n",
    "\n",
    "# # 训练模型\n",
    "# history_callback_2 = model.fit(train_tensors, train_targets, validation_split=0.2,\n",
    "#          epochs=epochs, batch_size=batch_size,callbacks=callback_lists,shuffle='True',verbose=1)\n",
    "\n",
    "history_callback_2 = model.fit(train_tensors, train_targets, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=0.2, shuffle='True', verbose=1, callbacks=callback_lists)\n",
    "\n",
    "\n",
    "endtime = time.strftime('end time:%Y-%m-%d %H:%M:%S', time.localtime()) \n",
    "print (endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示logloss趋势\n",
    "show_logloss(history_callback_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 测试模型\n",
    "\n",
    "在测试数据集上试用训练后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    " \n",
    "\n",
    "## 加载具有最好验证loss的模型\n",
    "# model.load_weights(Inceptionfile_hdf5)\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_tensors, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "# \n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(testnew_path, (224, 224), shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred.csv', index=None)\n",
    "df.head(10)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import *\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# def get_image(index):\n",
    "#     img = cv2.imread(testnew_path+'/test/%d.jpg' % index)\n",
    "#     img = cv2.resize(img,(299,299))\n",
    "#     img.astype(np.float32)\n",
    "#     img = img / 255.0\n",
    "#     return img\n",
    "\n",
    "# test_num = 12500\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# for i in range(20):\n",
    "#     x = get_image(random.randint(1, test_num))\n",
    "#     prediction = model.predict(np.expand_dims(x[0][0], axis=1))[0]\n",
    "    \n",
    "#     plt.subplot(4, 5, i+1)\n",
    "#     if prediction < 0.5:\n",
    "#         plt.title('cat %.2f%%' % (100 - prediction*100))\n",
    "#     else:\n",
    "#         plt.title('dog %.2f%%' % (prediction*100))\n",
    "    \n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(x[:,:,::-1]) # convert BGR to RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 提交 kaggle测试得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c dogs-vs-cats-redux-kernels-edition -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
